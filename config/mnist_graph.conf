#!/usr/bin/env python2

# Copyright (c) 2018 Salim Arslan <salim.arslan@imperial.ac.uk>
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

import os, time, datetime

# Environmental variables
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

# Data loading setup 
data_dir 		= './data/MNIST_data'
graph_file 		= data_dir + '/graph.pkl' # defaut None

# Graph/data parameters
coarsening_levels       = 4 # 4 allows two max-pooling of size 4 x 4, if 0 no coarsening 
number_edges          	= 8 # to define neighbourhood
metric                	= 'euclidean' # measure similiarty in graph
d 		     	= 1 # dimensionality of signals

# Model parameters
num_classes 		= 10  # number of classes
conv_depth 		= [1, 1, 1] # how many conv layers before applying max pooling?
filters 		= [32, 64, 128] # number of filters per layer
K_order 		= [25, 25, 25] # list of polynomial orders, i.e. filter sizes or number of hopes
strides 		= [4, 4, 1] # pooling size per layer (should be 1 - no pooling or a power of 2 - reduction by 2 at each coarser level)
bias			= 'b1relu'  # type of bias to use, 'b1' for one bias per filter or 'b2' for one bias per vertex per filter
pool			= 'mpool1' # pooling, 'mpool' for max pooling or 'apool' for average pooling
dropout			= 0.5 # dropout for fc layers, probability to keep hidden neurons (no dropout with 1)
filt 			= 'chebyshev5'

# Learning parameters
learning_rate		= 0.001 # learning rate -> reduce learning rate proportional to the capacity of the network (higher capacity/lower lr)
momentum 		= 0 # for momentum optimizer, 0 for adam
regularization 		= 5e-4 # weight decay
decay_rate 		= 1 #(default=1, no change) reduce learning rate by a factor of 0.5 every time validation accuracy drops in two consecutive eval steps
eval_frequency 		= 100 # evaluate model every this many steps
batch_size 		= 100 # train batch size
num_steps		= 2000 # number of steps (each step trains for a mini-batch)

# Logging
log_dir 		= './log/mnist_graph_model_' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M') # for model checkpoints and learning monitoring output







